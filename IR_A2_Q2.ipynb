{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7q3q7zDVLX5",
        "outputId": "b36b3ed3-6887-4ee6-fa1d-c584196ee77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-17-5d865191cb7e>:39: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text,'html.parser').get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 preprocessed reviews:\n",
            "['love', 'vintag', 'spring', 'vintag', 'strat', 'good', 'tension', 'great', 'stabil', 'float', 'bridg', 'want', 'spring', 'way', 'go']\n",
            "['work', 'great', 'guitar', 'bench', 'mat', 'rug', 'enough', 'abus', 'take', 'care', 'take', 'care', 'make', 'organ', 'workspac', 'much', 'easier', 'becaus', 'screw', 'wo', 'n', 't', 'roll', 'around', 'color', 'good']\n",
            "['use', 'everyth', 'acoust', 'bass', 'ukulel', 'know', 'smaller', 'model', 'avail', 'uke', 'violin', 'etc', 'n', 't', 'yet', 'order', 'work', 'smaller', 'instrument', 'one', 'doe', 'n', 't', 'extend', 'foot', 'maximum', 'width', 're', 'gentl', 'instrument', 'grippi', 'materi', 'keep', 'secur', 'greatest', 'benefit', 'ha', 'write', 'music', 'comput', 'need', 'set', 'guitar', 'use', 'keyboard', 'mous', 'easier', 'hang', 'stand', 'sever', 'gave', 'one', 'friend', 'christma', 'well', 've', 'use', 'mine', 'stage', 'fold', 'small', 'enough', 'fit', 'right', 'gig', 'bag']\n",
            "['great', 'price', 'good', 'qualiti', 'n', 't', 'quit', 'match', 'radiu', 'sound', 'hole', 'wa', 'close', 'enough']\n",
            "['bought', 'thi', 'bass', 'split', 'time', 'primari', 'bass', 'dean', 'edg', 'thi', 'might', 'win', 'bass', 'boost', 'outstand', 'activ', 'pickup', 'realli', 'allow', 'adjust', 'sound', 'want', 'recommend', 'thi', 'anyon', 're', 'beginn', 'like', 'wa', 'long', 'ago', 's', 'excel', 'bass', 'start', 're', 'tour', 'and', 'or', 'music', 'make', 'money', 'thi', 'bass', 'beati', 'stage', 'color', 'bit', 'darker', 'pictur', 'around', 'thi', 'great', 'buy']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/A2_Data.csv')\n",
        "\n",
        "# fetching the reviews from the csv file (3rd column)\n",
        "rev_text = data.iloc[:, 2].tolist()\n",
        "\n",
        "# Initialize WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Initialize Porter stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# preprocessing function\n",
        "def preprocess(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    # handling NaN reviews\n",
        "    if pd.isnull(text):\n",
        "        return\n",
        "\n",
        "    # lowercase reviews\n",
        "    text = text.lower()\n",
        "\n",
        "    # parse HTML content and extract text\n",
        "    text = BeautifulSoup(text,'html.parser').get_text()\n",
        "\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "    # removing stopwords\n",
        "    filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n",
        "\n",
        "    # removing punctuation\n",
        "    non_punct_tokens = tokenizer.tokenize(' '.join(filtered_tokens))\n",
        "\n",
        "    # removing blank space tokens\n",
        "    cleaned_tokens = [word for word in non_punct_tokens if not word.isspace()]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# perform preprocessing on each review\n",
        "preprocessed_reviews = [preprocess(review) for review in rev_text]\n",
        "\n",
        "# create a new DataFrame with original columns and preprocessed reviews\n",
        "new_data = data.copy()\n",
        "new_data['Preprocessed_Review'] = preprocessed_reviews\n",
        "\n",
        "# save the new DataFrame as a pickle file\n",
        "fp = \"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/preprocessed_data.pkl\"\n",
        "with open(fp, \"wb\") as f:\n",
        "    pickle.dump(new_data, f)\n",
        "\n",
        "# print the first 10 preprocessed reviews\n",
        "print(\"First 5 preprocessed reviews:\")\n",
        "for i in range(5):\n",
        "    print(preprocessed_reviews[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def create_vocab(preprocessed_docs):\n",
        "    unique_words = set()\n",
        "    if isinstance(preprocessed_docs, list):\n",
        "        for doc in preprocessed_docs:\n",
        "            if doc is not None:\n",
        "                # iterate through each word in the document\n",
        "                for word in doc:\n",
        "                    # add unique words to the set\n",
        "                    unique_words.add(word)\n",
        "        # sort unique words alphabetically and assign index to each word\n",
        "        unique_words = sorted(list(unique_words))\n",
        "        vocabulary = {word: idx for idx, word in enumerate(unique_words)}\n",
        "        return vocabulary\n",
        "    else:\n",
        "        print(\"incorrect format\")\n",
        "\n",
        "def calculate_idf(unique_words, preprocessed_docs):\n",
        "    idf_dictionary = {}\n",
        "    num_docs = len(preprocessed_docs)\n",
        "    for word in unique_words:\n",
        "        # count the number of docs containing the word\n",
        "        count = sum(1 for doc in preprocessed_docs if doc and word in doc)\n",
        "        # compute IDF value for each token\n",
        "        idf_dictionary[word] = float(1 + math.log((num_docs + 1) / (count + 1)))\n",
        "    return idf_dictionary\n",
        "\n",
        "def convert(preprocessed_docs, vocabulary, idf_dictionary):\n",
        "    if isinstance(preprocessed_docs, list):\n",
        "        tfidf_matrix = np.zeros((len(preprocessed_docs), len(vocabulary)))\n",
        "        for idx, doc in enumerate(preprocessed_docs):\n",
        "            if doc is not None:\n",
        "                # compute tf for each word\n",
        "                word_frequency = dict(Counter(doc))\n",
        "                for word, freq in word_frequency.items():\n",
        "                    col_index = vocabulary.get(word, -1)\n",
        "                    if col_index != -1:\n",
        "                        # calculate TF-IDF value\n",
        "                        tf = freq / float(len(preprocessed_docs))\n",
        "                        idf_value = idf_dictionary[word]\n",
        "                        tfidf_matrix[idx, col_index] = tf * idf_value\n",
        "        # L2 normalization\n",
        "        norms = np.linalg.norm(tfidf_matrix, axis=1)[:, np.newaxis]\n",
        "        # find indices where norms are zero\n",
        "        zero_indices = np.where(norms == 0)[0]\n",
        "        # replace zero norms with 1 to avoid division by zero\n",
        "        norms[zero_indices] = 1\n",
        "        # replace NaN norms with 0\n",
        "        norms[np.isnan(norms)] = 0\n",
        "        tfidf_matrix /= norms\n",
        "        return tfidf_matrix\n",
        "    else:\n",
        "        print(\"incorrect format\")\n",
        "\n",
        "# load the data from file\n",
        "data_frame = pd.read_pickle(\"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/preprocessed_data.pkl\")\n",
        "\n",
        "# fetch the preprocessed reviews from the 'Preprocessed_Review' column\n",
        "preprocessed_docs = data_frame['Preprocessed_Review'].tolist()\n",
        "\n",
        "# Step 1: create the vocabulary from a dataset of documents\n",
        "vocab = create_vocab(preprocessed_docs)\n",
        "\n",
        "# Step 2: calculate IDF\n",
        "idf_dict = calculate_idf(list(vocab.keys()), preprocessed_docs)\n",
        "\n",
        "# Step 3: convert to TF-IDF matrix\n",
        "tfidf_matrix = convert(preprocessed_docs, vocab, idf_dict)\n",
        "\n",
        "tfidf_vectors = [','.join(map(str, row)) for row in tfidf_matrix]\n",
        "\n",
        "# add the merged TF-IDF vectors as a new column in the DataFrame\n",
        "data_frame['TF-IDF'] = tfidf_vectors\n",
        "\n",
        "# save the modified DataFrame with TF-IDF vectors as a new pickle file\n",
        "data_frame.to_pickle(\"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/tfidf.pkl\")\n"
      ],
      "metadata": {
        "id": "ppemfRYcVcB_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "review_idx = 0  # Index of the first review\n",
        "review = preprocessed_reviews[review_idx]\n",
        "for word in review:\n",
        "    if word in vocab:\n",
        "        idx = vocab[word]\n",
        "        tf_idf = tfidf_matrix[review_idx, idx]\n",
        "        print(f\"Review {review_idx + 1}, Word: '{word}', TF-IDF: {tf_idf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ix1LM8Iw91S",
        "outputId": "c8820964-7ccb-4d9f-9fdf-27460ffcb795"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1, Word: 'love', TF-IDF: 0.1476\n",
            "Review 1, Word: 'vintag', TF-IDF: 0.4783\n",
            "Review 1, Word: 'spring', TF-IDF: 0.5616\n",
            "Review 1, Word: 'vintag', TF-IDF: 0.4783\n",
            "Review 1, Word: 'strat', TF-IDF: 0.1976\n",
            "Review 1, Word: 'good', TF-IDF: 0.1241\n",
            "Review 1, Word: 'tension', TF-IDF: 0.2751\n",
            "Review 1, Word: 'great', TF-IDF: 0.1028\n",
            "Review 1, Word: 'stabil', TF-IDF: 0.2751\n",
            "Review 1, Word: 'float', TF-IDF: 0.3142\n",
            "Review 1, Word: 'bridg', TF-IDF: 0.2097\n",
            "Review 1, Word: 'want', TF-IDF: 0.1516\n",
            "Review 1, Word: 'spring', TF-IDF: 0.5616\n",
            "Review 1, Word: 'way', TF-IDF: 0.1736\n",
            "Review 1, Word: 'go', TF-IDF: 0.1476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd1\n",
        "data = pd1.read_pickle(\"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/tfidf.pkl\")\n",
        "\n",
        "\n",
        "# print first 5 rows along with column names\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88_wKHZb_U30",
        "outputId": "673727b1-43dc-4a25-9aba-4ab9a2d50085"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                              Image  \\\n",
            "0        3452  ['https://images-na.ssl-images-amazon.com/imag...   \n",
            "1        1205  ['https://images-na.ssl-images-amazon.com/imag...   \n",
            "2        1708  ['https://images-na.ssl-images-amazon.com/imag...   \n",
            "3        2078  ['https://images-na.ssl-images-amazon.com/imag...   \n",
            "4         801  ['https://images-na.ssl-images-amazon.com/imag...   \n",
            "\n",
            "                                         Review Text  \\\n",
            "0  Loving these vintage springs on my vintage str...   \n",
            "1  Works great as a guitar bench mat. Not rugged ...   \n",
            "2  We use these for everything from our acoustic ...   \n",
            "3  Great price and good quality.  It didn't quite...   \n",
            "4  I bought this bass to split time as my primary...   \n",
            "\n",
            "                                 Preprocessed_Review  \\\n",
            "0  [love, vintag, spring, vintag, strat, good, te...   \n",
            "1  [work, great, guitar, bench, mat, rug, enough,...   \n",
            "2  [use, everyth, acoust, bass, ukulel, know, sma...   \n",
            "3  [great, price, good, qualiti, n, t, quit, matc...   \n",
            "4  [bought, thi, bass, split, time, primari, bass...   \n",
            "\n",
            "                                              TF-IDF  \n",
            "0  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n",
            "1  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n",
            "2  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n",
            "3  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n",
            "4  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import nltk\n",
        "import gzip\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "with open(\"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/extracted_features.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Convert NumPy array to DataFrame\n",
        "data_df = pd.DataFrame(data)\n",
        "\n",
        "reviews = data_df['review']\n",
        "\n",
        "# Initialize WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Initialize Porter stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# preprocessing function\n",
        "def preprocess(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    # handling NaN reviews\n",
        "    if pd.isnull(text):\n",
        "        return\n",
        "\n",
        "    # lowercase reviews\n",
        "    text = text.lower()\n",
        "\n",
        "    # parse HTML content and extract text\n",
        "    text = BeautifulSoup(text,'html.parser').get_text()\n",
        "\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "    # removing stopwords\n",
        "    filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n",
        "\n",
        "    # removing punctuation\n",
        "    non_punct_tokens = tokenizer.tokenize(' '.join(filtered_tokens))\n",
        "\n",
        "    # removing blank space tokens\n",
        "    cleaned_tokens = [word for word in non_punct_tokens if not word.isspace()]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# perform preprocessing on each review\n",
        "preprocessed_reviews = [preprocess(review) for review in reviews]\n",
        "\n",
        "# create a new DataFrame with original columns and preprocessed reviews\n",
        "data_df['Preprocessed_Review'] = preprocessed_reviews\n",
        "\n",
        "# save the new DataFrame as a pickle file\n",
        "fp = \"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/new_preprocessed_data.pkl\"\n",
        "with open(fp, \"wb\") as f:\n",
        "    pickle.dump(data_df, f)\n",
        "\n",
        "# print the first 10 preprocessed reviews\n",
        "print(\"First 50 preprocessed reviews:\")\n",
        "for i in range(50):\n",
        "    print(preprocessed_reviews[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyRjOd85-yt6",
        "outputId": "0925c2c3-dcbe-4767-9784-98b51178749b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-21-cc1cbf23c7ed>:42: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text,'html.parser').get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 50 preprocessed reviews:\n",
            "['love', 'vintag', 'spring', 'vintag', 'strat', 'good', 'tension', 'great', 'stabil', 'float', 'bridg', 'want', 'spring', 'way', 'go']\n",
            "['work', 'great', 'guitar', 'bench', 'mat', 'rug', 'enough', 'abus', 'take', 'care', 'take', 'care', 'make', 'organ', 'workspac', 'much', 'easier', 'becaus', 'screw', 'wo', 'n', 't', 'roll', 'around', 'color', 'good']\n",
            "['work', 'great', 'guitar', 'bench', 'mat', 'rug', 'enough', 'abus', 'take', 'care', 'take', 'care', 'make', 'organ', 'workspac', 'much', 'easier', 'becaus', 'screw', 'wo', 'n', 't', 'roll', 'around', 'color', 'good']\n",
            "['work', 'great', 'guitar', 'bench', 'mat', 'rug', 'enough', 'abus', 'take', 'care', 'take', 'care', 'make', 'organ', 'workspac', 'much', 'easier', 'becaus', 'screw', 'wo', 'n', 't', 'roll', 'around', 'color', 'good']\n",
            "['use', 'everyth', 'acoust', 'bass', 'ukulel', 'know', 'smaller', 'model', 'avail', 'uke', 'violin', 'etc', 'n', 't', 'yet', 'order', 'work', 'smaller', 'instrument', 'one', 'doe', 'n', 't', 'extend', 'foot', 'maximum', 'width', 're', 'gentl', 'instrument', 'grippi', 'materi', 'keep', 'secur', 'greatest', 'benefit', 'ha', 'write', 'music', 'comput', 'need', 'set', 'guitar', 'use', 'keyboard', 'mous', 'easier', 'hang', 'stand', 'sever', 'gave', 'one', 'friend', 'christma', 'well', 've', 'use', 'mine', 'stage', 'fold', 'small', 'enough', 'fit', 'right', 'gig', 'bag']\n",
            "['great', 'price', 'good', 'qualiti', 'n', 't', 'quit', 'match', 'radiu', 'sound', 'hole', 'wa', 'close', 'enough']\n",
            "['bought', 'thi', 'bass', 'split', 'time', 'primari', 'bass', 'dean', 'edg', 'thi', 'might', 'win', 'bass', 'boost', 'outstand', 'activ', 'pickup', 'realli', 'allow', 'adjust', 'sound', 'want', 'recommend', 'thi', 'anyon', 're', 'beginn', 'like', 'wa', 'long', 'ago', 's', 'excel', 'bass', 'start', 're', 'tour', 'and', 'or', 'music', 'make', 'money', 'thi', 'bass', 'beati', 'stage', 'color', 'bit', 'darker', 'pictur', 'around', 'thi', 'great', 'buy']\n",
            "['s', 'toy', 'side', 'instrument', 'side', 'made', 'indonesia', 'pro', 'tune', 'fun', 'play', 'small', 'easi', 'carri', 'around', 'solid', 'built', 'cheap', 'con', 'ca', 'n', 't', 'handl', 'fortissimo', 'loud', 'note']\n",
            "['s', 'toy', 'side', 'instrument', 'side', 'made', 'indonesia', 'pro', 'tune', 'fun', 'play', 'small', 'easi', 'carri', 'around', 'solid', 'built', 'cheap', 'con', 'ca', 'n', 't', 'handl', 'fortissimo', 'loud', 'note']\n",
            "['s', 'toy', 'side', 'instrument', 'side', 'made', 'indonesia', 'pro', 'tune', 'fun', 'play', 'small', 'easi', 'carri', 'around', 'solid', 'built', 'cheap', 'con', 'ca', 'n', 't', 'handl', 'fortissimo', 'loud', 'note']\n",
            "['s', 'toy', 'side', 'instrument', 'side', 'made', 'indonesia', 'pro', 'tune', 'fun', 'play', 'small', 'easi', 'carri', 'around', 'solid', 'built', 'cheap', 'con', 'ca', 'n', 't', 'handl', 'fortissimo', 'loud', 'note']\n",
            "['absolut', 'best', 'guitar', 'hanger', 'market', 'beat', 'thi', 'price', 'buy', 'still', 'thi', 'cheap']\n",
            "['great', 'nylon', 'string', 'expect', 'work', 'fine', 'daughter', 's', 'mini', 'classic', 'guitar', 'good', 'qualiti', 'wise', 'much', 'expens', 'string', 've', 'purchas', 'past', 'much', 'softer', 'touch', 'easier', 'learn', 'steel', 'string', 'acoust', 'guitar', 'came', 'label', 'note', 'name', 'string', 'number', 'made', 'put', 'easi', 'quick']\n",
            "['great', 'nylon', 'string', 'expect', 'work', 'fine', 'daughter', 's', 'mini', 'classic', 'guitar', 'good', 'qualiti', 'wise', 'much', 'expens', 'string', 've', 'purchas', 'past', 'much', 'softer', 'touch', 'easier', 'learn', 'steel', 'string', 'acoust', 'guitar', 'came', 'label', 'note', 'name', 'string', 'number', 'made', 'put', 'easi', 'quick']\n",
            "['great', 'nylon', 'string', 'expect', 'work', 'fine', 'daughter', 's', 'mini', 'classic', 'guitar', 'good', 'qualiti', 'wise', 'much', 'expens', 'string', 've', 'purchas', 'past', 'much', 'softer', 'touch', 'easier', 'learn', 'steel', 'string', 'acoust', 'guitar', 'came', 'label', 'note', 'name', 'string', 'number', 'made', 'put', 'easi', 'quick']\n",
            "['great', 'nylon', 'string', 'expect', 'work', 'fine', 'daughter', 's', 'mini', 'classic', 'guitar', 'good', 'qualiti', 'wise', 'much', 'expens', 'string', 've', 'purchas', 'past', 'much', 'softer', 'touch', 'easier', 'learn', 'steel', 'string', 'acoust', 'guitar', 'came', 'label', 'note', 'name', 'string', 'number', 'made', 'put', 'easi', 'quick']\n",
            "['bought', 'thi', 'stand', 'church', 'becaus', 'bless', 'soprano', 'alto', 'sax', 'reali', 'good', 'reliabl', 'fit', 'front', 'poutch', 'sax', 'case', 'problem', 'alto', 'case', 'ha', 'exstra', 'poutch', 'front', 'one', 'littl', 'thing', 'peg', 'fit', 'soprano', 'littl', 'lose', 'still', 'safe', 'soprano', 'sax']\n",
            "['bought', 'thi', 'stand', 'church', 'becaus', 'bless', 'soprano', 'alto', 'sax', 'reali', 'good', 'reliabl', 'fit', 'front', 'poutch', 'sax', 'case', 'problem', 'alto', 'case', 'ha', 'exstra', 'poutch', 'front', 'one', 'littl', 'thing', 'peg', 'fit', 'soprano', 'littl', 'lose', 'still', 'safe', 'soprano', 'sax']\n",
            "['bought', 'thi', 'stand', 'church', 'becaus', 'bless', 'soprano', 'alto', 'sax', 'reali', 'good', 'reliabl', 'fit', 'front', 'poutch', 'sax', 'case', 'problem', 'alto', 'case', 'ha', 'exstra', 'poutch', 'front', 'one', 'littl', 'thing', 'peg', 'fit', 'soprano', 'littl', 'lose', 'still', 'safe', 'soprano', 'sax']\n",
            "['bought', 'thi', 'stand', 'church', 'becaus', 'bless', 'soprano', 'alto', 'sax', 'reali', 'good', 'reliabl', 'fit', 'front', 'poutch', 'sax', 'case', 'problem', 'alto', 'case', 'ha', 'exstra', 'poutch', 'front', 'one', 'littl', 'thing', 'peg', 'fit', 'soprano', 'littl', 'lose', 'still', 'safe', 'soprano', 'sax']\n",
            "['awesom', 'stand', 'tip', 'bottom', 'part', 'support', 'guitar', 'weird', 'angl', 'arriv', 'make', 'guitar', 'slide', 'back', 'becom', 'almost', '100', 'vertic', 'solv', 'thi', 'assembl', 'product', 'put', 'pressur', 'support', 'frame', 'make', 'bend', 'littl', 'guitar', 'sit', 'perfectli', 'check', 'photo']\n",
            "['awesom', 'stand', 'tip', 'bottom', 'part', 'support', 'guitar', 'weird', 'angl', 'arriv', 'make', 'guitar', 'slide', 'back', 'becom', 'almost', '100', 'vertic', 'solv', 'thi', 'assembl', 'product', 'put', 'pressur', 'support', 'frame', 'make', 'bend', 'littl', 'guitar', 'sit', 'perfectli', 'check', 'photo']\n",
            "['awesom', 'stand', 'tip', 'bottom', 'part', 'support', 'guitar', 'weird', 'angl', 'arriv', 'make', 'guitar', 'slide', 'back', 'becom', 'almost', '100', 'vertic', 'solv', 'thi', 'assembl', 'product', 'put', 'pressur', 'support', 'frame', 'make', 'bend', 'littl', 'guitar', 'sit', 'perfectli', 'check', 'photo']\n",
            "['gave', 'cheaper', 'neck', 'shot', 'blown', 'away', 'look', 'thi', 'neck', 'stain', 'use', 'enough', 'highlight', 'grain', 'nice', 'clear', 'coat', 'darn', 'near', 'perfect', 'figur', 'lot', 'time', 'would', 'dure', 'adjust', 'phase', 'build', 'noth', 'el', 'done', 'onc', 'neck', 'wa', 'set', 'bolt', 'pleasant', 'surpris', 'sinc', 'past', 'build', 'd', 'spent', 'hour', 'get', 'set', 've', 'use', 'fender', 'warmoth', 'befor', 'cost', '3', 'time', 'thi', 'one', 'proof', 'pud', 'though', 'thi', 'neck', 'ha', 'thi', 'guitar', 'year', 'problem', 'use', 'thi', 'guitar', 'back', 'play', 'quit', 'frequent', 'gig', 'buy']\n",
            "['gave', 'cheaper', 'neck', 'shot', 'blown', 'away', 'look', 'thi', 'neck', 'stain', 'use', 'enough', 'highlight', 'grain', 'nice', 'clear', 'coat', 'darn', 'near', 'perfect', 'figur', 'lot', 'time', 'would', 'dure', 'adjust', 'phase', 'build', 'noth', 'el', 'done', 'onc', 'neck', 'wa', 'set', 'bolt', 'pleasant', 'surpris', 'sinc', 'past', 'build', 'd', 'spent', 'hour', 'get', 'set', 've', 'use', 'fender', 'warmoth', 'befor', 'cost', '3', 'time', 'thi', 'one', 'proof', 'pud', 'though', 'thi', 'neck', 'ha', 'thi', 'guitar', 'year', 'problem', 'use', 'thi', 'guitar', 'back', 'play', 'quit', 'frequent', 'gig', 'buy']\n",
            "['gave', 'cheaper', 'neck', 'shot', 'blown', 'away', 'look', 'thi', 'neck', 'stain', 'use', 'enough', 'highlight', 'grain', 'nice', 'clear', 'coat', 'darn', 'near', 'perfect', 'figur', 'lot', 'time', 'would', 'dure', 'adjust', 'phase', 'build', 'noth', 'el', 'done', 'onc', 'neck', 'wa', 'set', 'bolt', 'pleasant', 'surpris', 'sinc', 'past', 'build', 'd', 'spent', 'hour', 'get', 'set', 've', 'use', 'fender', 'warmoth', 'befor', 'cost', '3', 'time', 'thi', 'one', 'proof', 'pud', 'though', 'thi', 'neck', 'ha', 'thi', 'guitar', 'year', 'problem', 'use', 'thi', 'guitar', 'back', 'play', 'quit', 'frequent', 'gig', 'buy']\n",
            "['dunlop', 'cbm95', 'cri', 'babi', 'mini', 'wah', 'better', 'put', 'pedal', 'board', 'gcb95', 'true', 'bypass', 'three', 'mode', 'support', 'much', 'better', 'gcb95', 'would', 'nice', 'support', 'led', 'team', 'kill']\n",
            "['absolut', 'amaz', 'think', 'direct', 'knock', 'off', 'seymour', 'duncan', 'sh', '2', 'jazz', 'model', 'humbuck', 'pickup', 'see', 'littl', 'hole', 'pup', 'power', 'bright', 'sound', 'nois', 'old', 'johnson', 'lp', 'ex', 'emg', 'ish', 'pickup', 'fulli', 'awak', 'angri', 'good', 'way', 'great', 'qualiti', 'thank', 'everyon', 'good', 'review']\n",
            "['pick', 'holder', 'doe', 'suppos', 'mainli', 'use', 'fender', 'medium', 'pick', 'fender', 'heavi', 'pick', 'around', 'n', 't', 'ani', 'thin', 'pick', 'itll', 'hold', '6', 'heavi', 'pick', '9', 'medium', 'pick', 'would', 'imagin', 'would', 'hold', '11', '12', 'thin', 'pick']\n",
            "['kinda', 'flimsi', 'oit', 'doe', 'job']\n",
            "['kinda', 'flimsi', 'oit', 'doe', 'job']\n",
            "['kinda', 'flimsi', 'oit', 'doe', 'job']\n",
            "['kinda', 'flimsi', 'oit', 'doe', 'job']\n",
            "['thi', 'fine', 'eq', 'sound', 'good', 'm', 'think', 'get', 'anoth', 'one', 'right', 'left', 'speaker']\n",
            "['thi', 'fine', 'eq', 'sound', 'good', 'm', 'think', 'get', 'anoth', 'one', 'right', 'left', 'speaker']\n",
            "['period', 'month', 'one', 'two', 'cabl', 'fail', 'unplug', 'actual', 'quarter', 'inch', 'end', 'broke', 'wa', 'pull', 'cabl', 'normal', 'end', 'practic', 'show', 'got', 'stuck', 'amp', 'thank', 'thi', 'spend', 'money', 'time', 'n', 't', 'get', 'amp', 'fix', 'broken', 'cabl', 'buy', 'better', 'stronger', 'durabl', 'cabl', 'wo', 'n', 't', 'break', 'pull', 'amp']\n",
            "['period', 'month', 'one', 'two', 'cabl', 'fail', 'unplug', 'actual', 'quarter', 'inch', 'end', 'broke', 'wa', 'pull', 'cabl', 'normal', 'end', 'practic', 'show', 'got', 'stuck', 'amp', 'thank', 'thi', 'spend', 'money', 'time', 'n', 't', 'get', 'amp', 'fix', 'broken', 'cabl', 'buy', 'better', 'stronger', 'durabl', 'cabl', 'wo', 'n', 't', 'break', 'pull', 'amp']\n",
            "['thi', 'gator', 'guitar', 'hanger', 'sturdi', 'attract', 'easi', 'instal', 'come', 'wall', 'anchor', 'drywal', 'long', 'screw', 'hanger', 'rubber', 'coat', 'prevent', 'finish', 'damag', 'instruct', 'simpl', 'thi', 'instal', '5', 'minut', 'screw', 'visibl', 'onc', 'unit', 'wall', 'wa', 'initi', 'worri', 'guitar', 'would', 'hang', 'close', 'wall', 'howev', 'good', '3', '5', 'away', 'take', 'oomph', 'get', 'actual', 'hanger', 'prong', 'parallel', 'floor', 'certainli', 'feel', 'solidli', 'lock', 'wa', 'fortun', 'enough', 'locat', 'stud', 'wa', 'avail', 'would', 'highli', 'recommend', 'use', 'stud', 'instal', 'howev', '2', 'heavi', 'duti', 'screw', 'in', 'wall', 'anchor', 'also', 'come', 'unit', 'drywal', 'mount', 'much', 'prefer', 'thi', 'clean', 'look', 'guitar', 'floor', 'stand', 'corner', 'tend', 'gather', 'dust', 'dog', 'hair', 'ugh']\n",
            "['bought', 'thi', 'item', 'becaus', 'good', 'review', 'wa', 'cost', 'effect', 'compar', 'top', 'name', 'brand', 'far', 'onli', 'use', 'thi', 'item', 'practic', 'set', 'doe', 'exactli', 'state', 'notic', 'product', 'left', 'sever', 'hour', 'get', 'littl', 'warm', 'doe', 'n', 't', 'seem', 'ani', 'sort', 'issu', '12v', '18v', 'allow', 'easi', 'hookup', 'oddbal', 'pedal', 'make', 'sure', 'n', 't', 'plug', '9v', 'pedal', 'one', 'slot', 'would', 'recommend', 'thi', 'item', 'anybodi', 'compil', 'first', 'pedal', 'board', 'know', 'pop', '9v', 'batteri', 'pedal', 'away', 'go', 'run', 'cost', 'comparison', 'thi', 'power', 'suppli', '9v', 'batteri', 'make', 'pretti', 'obviou', 'thi', 'power', 'suppli', 'worth', 'everi', 'penni', 'plu', 'never', 'worri', 'dead', 'batteri', 'loop', 'ruin', 'gig']\n",
            "['realli', 'cant', 'beat', 'price', 'howev', 'would', 'use', 'realli', 'heavi', 'light', 'onli', 'rate', '50lb', 'anyway', 'led', 'fixtur', 'coupl', 'laser', 'attach', 'pictur', 'thi', 'perfect', 'heavi', 'scanner', 'larg', 'format', 'light', 'even', 'onli', 'use', 'one', '40', 'lb', 'would', 'use', 'onli', 'feel', 'secur', 'enough', 'coupl', 'smaller', 'light', 'fogger', 'buy', 'could', 'alway', 'use', 'entertain', 'center', 'mobil', 'dj', 'rig', 'sort', 'light', 'applic']\n",
            "['never', 'use', 'akg', 'microphon', 'befor', 'decid', 'take', 'chanc', 'thi', 'great', 'review', 'right', 'use', 'thi', 'record', 'taylor', '914ce', 'sound', 'wa', 'chrisp', 'great', 'mid', 'enough', 'low', 'overpow', 'sound', 'total', 'recommend', 'thi', 'vocal', 'acoust', 'guitar', 'record']\n",
            "['s', 'hard', 'give', 'le', '5', 'star', 'becaus', 'thi', 'light', 'amaz', 'valu', 'pack', 'seriou', 'punch', 'term', 'output', 'feel', 'quit', 'sturdi', 'ha', 'veri', 'thin', 'profil', 'hous', 'thick', 'rug', 'plastic', 'mount', 'hardwar', 'feel', 'smooth', 'easi', 'adjust', 'display', 'back', 'larger', 'light', 'thi', 'style', 'nice', 'configur', 'dmx', 'channel', 'control', 'mine', 'via', 'softwar', 'dmx', 'system', 'ca', 'n', 't', 'speak', 'built', 'in', 'mode', 'ha', 'work', 'flawlessli', 'dmx', 'environ', 'wa', 'curiou', 'thi', 'light', 'would', 'perform', 'vs', 'expens', '4', 'in', '1', 'style', 'surpris', 'color', 'mix', 'veri', 'smooth', 'ha', 'veri', 'litl', 'rainbow', 'effect', 'use', 'wash', 'uplight', 'recent', 'play', 'gig', 'wineri', 'use', 'thi', 'light', 'back', 'light', 'stage', 'thi', 'light', 'rear', 'left', 'attach', 'pictur', 'gbg', '8wx9', 'right', 'potenti', 'con', 'hard', 'wir', 'power', 'cord', 'somewhat', 'loud', 'fan', 'notic', 'live', 'music', 'context']\n",
            "['s', 'hard', 'give', 'le', '5', 'star', 'becaus', 'thi', 'light', 'amaz', 'valu', 'pack', 'seriou', 'punch', 'term', 'output', 'feel', 'quit', 'sturdi', 'ha', 'veri', 'thin', 'profil', 'hous', 'thick', 'rug', 'plastic', 'mount', 'hardwar', 'feel', 'smooth', 'easi', 'adjust', 'display', 'back', 'larger', 'light', 'thi', 'style', 'nice', 'configur', 'dmx', 'channel', 'control', 'mine', 'via', 'softwar', 'dmx', 'system', 'ca', 'n', 't', 'speak', 'built', 'in', 'mode', 'ha', 'work', 'flawlessli', 'dmx', 'environ', 'wa', 'curiou', 'thi', 'light', 'would', 'perform', 'vs', 'expens', '4', 'in', '1', 'style', 'surpris', 'color', 'mix', 'veri', 'smooth', 'ha', 'veri', 'litl', 'rainbow', 'effect', 'use', 'wash', 'uplight', 'recent', 'play', 'gig', 'wineri', 'use', 'thi', 'light', 'back', 'light', 'stage', 'thi', 'light', 'rear', 'left', 'attach', 'pictur', 'gbg', '8wx9', 'right', 'potenti', 'con', 'hard', 'wir', 'power', 'cord', 'somewhat', 'loud', 'fan', 'notic', 'live', 'music', 'context']\n",
            "['s', 'hard', 'give', 'le', '5', 'star', 'becaus', 'thi', 'light', 'amaz', 'valu', 'pack', 'seriou', 'punch', 'term', 'output', 'feel', 'quit', 'sturdi', 'ha', 'veri', 'thin', 'profil', 'hous', 'thick', 'rug', 'plastic', 'mount', 'hardwar', 'feel', 'smooth', 'easi', 'adjust', 'display', 'back', 'larger', 'light', 'thi', 'style', 'nice', 'configur', 'dmx', 'channel', 'control', 'mine', 'via', 'softwar', 'dmx', 'system', 'ca', 'n', 't', 'speak', 'built', 'in', 'mode', 'ha', 'work', 'flawlessli', 'dmx', 'environ', 'wa', 'curiou', 'thi', 'light', 'would', 'perform', 'vs', 'expens', '4', 'in', '1', 'style', 'surpris', 'color', 'mix', 'veri', 'smooth', 'ha', 'veri', 'litl', 'rainbow', 'effect', 'use', 'wash', 'uplight', 'recent', 'play', 'gig', 'wineri', 'use', 'thi', 'light', 'back', 'light', 'stage', 'thi', 'light', 'rear', 'left', 'attach', 'pictur', 'gbg', '8wx9', 'right', 'potenti', 'con', 'hard', 'wir', 'power', 'cord', 'somewhat', 'loud', 'fan', 'notic', 'live', 'music', 'context']\n",
            "['realli', 'love', 'fretwrap', 're', 'realli', 'useful', 'special', 'need', 'record', 'someth', 'want', 'realli', 'clean', 'sound', 'bass', 'player', 'love', 'slap', 'techniqu', 'got', 'first', 'set', 'like', 'much', 'end', 'buy', 'differ', 'color']\n",
            "['want', 'custom', 'fake', 'plastic', 'rock', 'band', 'guitar', 'beatl', 'rock', 'band', 'x360', 'wireless', 'rickenback', '325', 'guitar', 'control', 'inexpens', 'tune', 'peg', 'fit', 'bill', 'm', 'happi', 'end', 'result', 'pictur', 'finish', 'project', 'upload', 'upload', 'custom', 'imag', 'howev', 'need', 'tune', 'peg', 'real', 'guitar', 'would', 'sure', 'go', 'better', 'choic', 'pay', 'littl', 'better', 'qualiti']\n",
            "['want', 'custom', 'fake', 'plastic', 'rock', 'band', 'guitar', 'beatl', 'rock', 'band', 'x360', 'wireless', 'rickenback', '325', 'guitar', 'control', 'inexpens', 'tune', 'peg', 'fit', 'bill', 'm', 'happi', 'end', 'result', 'pictur', 'finish', 'project', 'upload', 'upload', 'custom', 'imag', 'howev', 'need', 'tune', 'peg', 'real', 'guitar', 'would', 'sure', 'go', 'better', 'choic', 'pay', 'littl', 'better', 'qualiti']\n",
            "['went', 'new', 'strat', 'day', 'white', 'strat', 'came', 'home', 'onli', 'reason', 'chang', 'string', 'time', 'anyth', 'el', 'nope']\n",
            "['perfect', 'fit', 'color', 'match', 'veri', 'close', 'great', 'product', 'ad', 'pickguard', 'le', 'paul', 'studio', 'delux']\n",
            "['perfect', 'fit', 'color', 'match', 'veri', 'close', 'great', 'product', 'ad', 'pickguard', 'le', 'paul', 'studio', 'delux']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def create_vocab(preprocessed_reviews):\n",
        "    uniq_words = set()\n",
        "    if isinstance(preprocessed_reviews, list):\n",
        "        for doc in preprocessed_reviews:\n",
        "            if doc is not None:\n",
        "                # iterate through each word in the document\n",
        "                for word in doc:\n",
        "                    # add unique words to the set\n",
        "                    uniq_words.add(word)\n",
        "        # sort unique words alphabetically and assign index to each word\n",
        "        uniq_words = sorted(list(uniq_words))\n",
        "        vocab = {word: idx for idx, word in enumerate(uniq_words)}\n",
        "        return vocab\n",
        "    else:\n",
        "        print(\"incorrect format\")\n",
        "\n",
        "def calculate_idf(uniq_words, preprocessed_reviews):\n",
        "    idf_dict = {}\n",
        "    num = len(preprocessed_reviews)\n",
        "    for word in uniq_words:\n",
        "        # count the number of docs containing the word\n",
        "        count = sum(1 for doc in preprocessed_reviews if doc and word in doc)\n",
        "        # compute IDF value for each token\n",
        "        idf_dict[word] = float(1 + math.log((num + 1) / (count + 1)))\n",
        "    return idf_dict\n",
        "\n",
        "def convert(preprocessed_reviews, vocab, idf_dict):\n",
        "    if isinstance(preprocessed_reviews, list):\n",
        "        tfidf_matrix = np.zeros((len(preprocessed_reviews), len(vocab)))\n",
        "        for idx, doc in enumerate(preprocessed_reviews):\n",
        "            if doc is not None:\n",
        "                # compute tf for each word\n",
        "                word_freq = dict(Counter(doc))\n",
        "                for word, freq in word_freq.items():\n",
        "                    col_index = vocab.get(word, -1)\n",
        "                    if col_index != -1:\n",
        "                        # calculate TF-IDF value\n",
        "                        tf = freq / float(len(preprocessed_reviews))\n",
        "                        idf_ = idf_dict[word]\n",
        "                        tfidf_matrix[idx, col_index] = tf * idf_\n",
        "        # L2 normalization\n",
        "        norms = np.linalg.norm(tfidf_matrix, axis=1)[:, np.newaxis]\n",
        "        # find indices where norms are zero\n",
        "        zero_indices = np.where(norms == 0)[0]\n",
        "        # replace zero norms with 1 to avoid division by zero\n",
        "        norms[zero_indices] = 1\n",
        "        # replace NaN norms with 0\n",
        "        norms[np.isnan(norms)] = 0\n",
        "        tfidf_matrix /= norms\n",
        "        return tfidf_matrix\n",
        "    else:\n",
        "        print(\"incorrect format\")\n",
        "\n",
        "# load the data from file\n",
        "data = pd.read_pickle(\"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/new_preprocessed_data.pkl\")\n",
        "\n",
        "# fetch the preprocessed reviews from the 'Preprocessed_Review' column\n",
        "preprocessed_reviews = data['Preprocessed_Review'].tolist()\n",
        "\n",
        "# Step 1: create the vocabulary from a dataset of documents\n",
        "vocab = create_vocab(preprocessed_reviews)\n",
        "\n",
        "# Step 2: calculate IDF\n",
        "idf_dict = calculate_idf(list(vocab.keys()), preprocessed_reviews)\n",
        "\n",
        "# Step 3: convert to TF-IDF matrix\n",
        "tfidf_matrix = convert(preprocessed_reviews, vocab, idf_dict)\n",
        "\n",
        "tfidf_vectors = [','.join(map(str, row)) for row in tfidf_matrix]\n",
        "print(tfidf_vectors)\n",
        "# Add the merged TF-IDF vectors as a new column in the DataFrame\n",
        "data['TF-IDF'] = tfidf_vectors\n",
        "\n",
        "\n",
        "\n",
        "review_idx = 0  # Index of the first review\n",
        "review = preprocessed_reviews[review_idx]\n",
        "for word in review:\n",
        "    if word in vocab:\n",
        "        idx = vocab[word]\n",
        "        tf_idf = tfidf_matrix[review_idx, idx]\n",
        "        print(f\"Review {review_idx + 1}, Word: '{word}', TF-IDF (Normalized): {tf_idf:.4f}\")\n",
        "\n",
        "\n",
        "# Save the modified DataFrame with TF-IDF vectors as a new pickle file\n",
        "data.to_pickle(\"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/new_tfidf.pkl\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJn09wox3nh2",
        "outputId": "47f74c85-a313-418c-b3a9-c5e0d65b54cb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd1\n",
        "data = pd1.read_pickle(\"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/new_tfidf.pkl\")\n",
        "\n",
        "\n",
        "# print first 5 rows along with column names\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz30znzfrTzO",
        "outputId": "3d62b7b1-d46c-42e3-9e27-adf87b6c3012"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id                                          image_url  \\\n",
            "0  3452.0  https://images-na.ssl-images-amazon.com/images...   \n",
            "1  1205.0  https://images-na.ssl-images-amazon.com/images...   \n",
            "2  1205.0  https://images-na.ssl-images-amazon.com/images...   \n",
            "3  1205.0  https://images-na.ssl-images-amazon.com/images...   \n",
            "4  1708.0  https://images-na.ssl-images-amazon.com/images...   \n",
            "\n",
            "                                              review  \\\n",
            "0  Loving these vintage springs on my vintage str...   \n",
            "1  Works great as a guitar bench mat. Not rugged ...   \n",
            "2  Works great as a guitar bench mat. Not rugged ...   \n",
            "3  Works great as a guitar bench mat. Not rugged ...   \n",
            "4  We use these for everything from our acoustic ...   \n",
            "\n",
            "                                 normalized_features  \\\n",
            "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "1  [0.0, 0.0, 0.0, 0.0, 0.33417895, 0.0, 0.0, 0.4...   \n",
            "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23952477...   \n",
            "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "\n",
            "                                 Preprocessed_Review  \\\n",
            "0  [love, vintag, spring, vintag, strat, good, te...   \n",
            "1  [work, great, guitar, bench, mat, rug, enough,...   \n",
            "2  [work, great, guitar, bench, mat, rug, enough,...   \n",
            "3  [work, great, guitar, bench, mat, rug, enough,...   \n",
            "4  [use, everyth, acoust, bass, ukulel, know, sma...   \n",
            "\n",
            "                                              TF-IDF  \n",
            "0  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n",
            "1  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n",
            "2  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n",
            "3  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n",
            "4  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_pickle(\"/content/drive/MyDrive/CSE508_Winter2024_A2_MT23029/new_tfidf.pkl\")\n",
        "\n",
        "\n",
        "# Print last column of the first row of the pickle file\n",
        "print(data.iloc[0, -1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjcymmEN5hrm",
        "outputId": "25e66682-ec8f-4733-97c3-6cebd4f43378"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20557553473680662,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28427901127422167,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14627089123137033,0.0,0.0,0.0,0.0,0.0,0.0,0.1188881587848887,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10147034861237052,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13914040355147725,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5987086343910708,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28015997293811506,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18918313706111564,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27637082498022947,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4617170140730656,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14511903202764162,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16847381303019088,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n"
          ]
        }
      ]
    }
  ]
}